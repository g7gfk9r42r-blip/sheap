# Universal Prospekt Scraper

Ein umfassender Scraper, der alle Prospekt-Dateien in `media/prospekte/` rekursiv verarbeitet und Angebote extrahiert.

## ğŸ¯ Features

- âœ… **Rekursive Ordner-Durchsuchung** - Verarbeitet alle Unterordner automatisch
- âœ… **Multi-Format Support** - PDF, HTML, JSON, TXT
- âœ… **Robuste Fehlerbehandlung** - Einzelne Fehler stoppen nicht die gesamte Verarbeitung
- âœ… **Metadaten-Tracking** - Speichert Informationen Ã¼ber alle verarbeiteten Dateien
- âœ… **Klar gekennzeichnete Ausgabe** - Dateien mit `_processed_` im Namen
- âœ… **Deduplizierung** - Entfernt doppelte Angebote automatisch
- âœ… **Detailliertes Logging** - Zeigt Fortschritt und Ergebnisse

## ğŸ“ Struktur

```
media/prospekte/
â”œâ”€â”€ aldi_nord/
â”‚   â””â”€â”€ *.pdf
â”œâ”€â”€ lidl/
â”‚   â””â”€â”€ *.pdf
â”œâ”€â”€ edeka/
â”‚   â”œâ”€â”€ edeka berlin/
â”‚   â”‚   â”œâ”€â”€ *.pdf
â”‚   â”‚   â”œâ”€â”€ *.html
â”‚   â”‚   â””â”€â”€ *.json
â”‚   â””â”€â”€ edeka mÃ¼nchen/
â”‚       â””â”€â”€ *.pdf
â””â”€â”€ ...
```

## ğŸš€ Verwendung

### Einmalige AusfÃ¼hrung

```bash
npm run process:all
```

Oder direkt:

```bash
npm run build && node scripts/process_all_prospekte.mjs
```

### WÃ¶chentliche AusfÃ¼hrung (Cron)

FÃ¼ge folgende Zeile zu deinem Crontab hinzu:

```bash
# Jeden Montag um 6:00 Uhr
0 6 * * 1 cd /path/to/server && npm run process:all >> logs/prospekt_scraper.log 2>&1
```

## ğŸ“‹ Ausgabe

FÃ¼r jeden Ordner wird eine JSON-Datei erstellt:

**Dateiname:** `{retailer}_{region}_processed_{weekKey}.json`

**Beispiel:** `edeka_berlin_processed_2025-W48.json`

### Struktur der Ausgabe-Datei

```json
{
  "metadata": {
    "retailer": "EDEKA",
    "region": "Berlin",
    "weekKey": "2025-W48",
    "year": 2025,
    "week": 48,
    "processedAt": "2025-11-25T10:30:00.000Z",
    "source": "prospekt-scraper",
    "version": "1.0.0",
    "totalFilesProcessed": 3,
    "successfulFiles": 3,
    "failedFiles": 0
  },
  "processedFiles": [
    {
      "path": "edeka/edeka berlin/kaufDA - EDEKA - Aktuelle Angebote.pdf",
      "type": "pdf",
      "success": true,
      "offersCount": 150,
      "processedAt": "2025-11-25T10:30:15.000Z"
    }
  ],
  "offers": [
    {
      "name": "Prodomo",
      "price": 6.99,
      "price_old": 10.49,
      "savings": 3.50
    }
  ]
}
```

## ğŸ”§ UnterstÃ¼tzte Formate

### PDF
- Extrahiert Text mit `pdf-parse`
- Erkennt Preise, Produktnamen, Rabatte
- UnterstÃ¼tzt verschiedene Layouts

### HTML
- Verarbeitet vollstÃ¤ndig gespeicherte HTML-Dateien (mit Assets)
- Nutzt `cheerio` fÃ¼r Parsing
- Extrahiert Angebote aus KaufDA-Format

### JSON
- UnterstÃ¼tzt verschiedene JSON-Formate:
  - Array von Angeboten
  - Objekt mit `offers` Array
  - Raw-Format mit `raw` Array
- Normalisiert alle Formate zu einheitlichem Format

### TXT
- Zeilenweise Verarbeitung
- Erkennt Preise im Format `X,XX â‚¬`
- Extrahiert Produktnamen

## âš™ï¸ Konfiguration

Der Scraper erkennt automatisch:
- **Retailer** aus Ordnernamen (z.B. `aldi_nord` â†’ `ALDI`)
- **Region** aus Unterordnernamen (z.B. `edeka berlin` â†’ `Berlin`)
- **Dateityp** aus Dateiendung

## ğŸ“Š Logging

Der Scraper gibt detaillierte Informationen aus:

```
ğŸš€ Universal Prospekt Scraper

ğŸ“‚ Prospekt-Verzeichnis: /path/to/media/prospekte

ğŸª EDEKA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ edeka berlin
  ğŸ“„ kaufDA - EDEKA - Aktuelle Angebote.pdf (pdf)
  âœ… 150 Angebote extrahiert, 1/1 Dateien erfolgreich
  ğŸ“‹ Gespeichert: edeka_berlin_processed_2025-W48.json

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š ZUSAMMENFASSUNG
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ Verarbeitete Ordner: 5
ğŸ“„ Gesamt Dateien: 12
âœ… Erfolgreich: 11
âŒ Fehlgeschlagen: 1
ğŸ“¦ Gesamt Angebote: 1250
```

## ğŸ›¡ï¸ Fehlerbehandlung

- **Einzelne Dateifehler** stoppen nicht die gesamte Verarbeitung
- **Fehlgeschlagene Dateien** werden in Metadaten dokumentiert
- **Detaillierte Fehlermeldungen** fÃ¼r Debugging
- **Fortsetzung nach Fehlern** - verarbeitet weiterhin andere Dateien

## ğŸ” Ãœbersprungene Dateien

Folgende Dateien werden automatisch Ã¼bersprungen:
- Bereits verarbeitete Dateien (`*_processed_*.json`, `*_final_*.json`)
- `_files` Ordner (HTML-Assets)
- `jsondateivoll` Dateien

## ğŸ’¡ Tipps

1. **VollstÃ¤ndige HTML-Dateien**: Speichere Prospekt-Seiten als "Webseite, vollstÃ¤ndig" (mit allen Assets)
2. **PDF bevorzugen**: PDFs liefern meist bessere Ergebnisse
3. **Mehrere Formate**: Wenn ein Format unvollstÃ¤ndig ist, hilft ein anderes Format
4. **WÃ¶chentliche AusfÃ¼hrung**: FÃ¼hre den Scraper jeden Montag aus, nachdem neue Prospekte hochgeladen wurden

## ğŸ› Troubleshooting

### Keine Angebote gefunden
- PrÃ¼fe, ob die Datei lesbar ist
- PrÃ¼fe das Dateiformat (PDF sollte Text enthalten, nicht nur Bilder)
- PrÃ¼fe die Logs fÃ¼r Fehlermeldungen

### Fehler beim Parsen
- PrÃ¼fe, ob die Datei korrekt formatiert ist
- PrÃ¼fe, ob alle AbhÃ¤ngigkeiten installiert sind (`npm install`)
- PrÃ¼fe die Logs fÃ¼r detaillierte Fehlermeldungen

### Langsame Verarbeitung
- PDF-Verarbeitung kann bei groÃŸen Dateien langsam sein
- HTML-Verarbeitung ist meist schneller
- JSON-Verarbeitung ist am schnellsten

## ğŸ“ Changelog

### Version 1.0.0
- Initiale Version
- UnterstÃ¼tzung fÃ¼r PDF, HTML, JSON, TXT
- Rekursive Ordner-Durchsuchung
- Metadaten-Tracking
- Deduplizierung

